{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T17:22:13.936129Z",
     "start_time": "2024-12-05T17:22:13.923469Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing_extensions import List, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T17:22:20.247898Z",
     "start_time": "2024-12-05T17:22:14.547028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_json_output(response: str):\n",
    "    json_match = re.search(r\"<output>(.*?)</output>\", response, re.DOTALL)\n",
    "    json_string = json_match.group(1).strip()\n",
    "    parsed_json = json.loads(json_string)\n",
    "    return parsed_json\n",
    "    \n",
    "def extract_str_output(response: str):\n",
    "    str_match = re.search(r\"<output>(.*?)</output>\", response, re.DOTALL)\n",
    "    str_string = str_match.group(1).strip()\n",
    "    return str_string\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "loader = PyPDFLoader(file_path=\"data/AML_IEEE_ACCESS_2024.pdf\", extract_images=True)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "docs = loader.load()\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "db = DocArrayInMemorySearch.from_documents(all_splits, embeddings)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"][-1].content)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "def evaluate_documents(state: MessagesState):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an expert document assessor tasked with determining the relevance of a retrieved document to a user's question. Your goal is to provide an accurate relevance assessment based on both keyword matches and semantic understanding.\n",
    "\n",
    "        First, carefully review the following information:\n",
    "        \n",
    "        1. Retrieved Documents:\n",
    "        <retrieved documents>\n",
    "        {documents}\n",
    "        </retrieved documents>\n",
    "        \n",
    "        2. Here is the user question:\n",
    "        <question>\n",
    "        {query}\n",
    "        </question>\n",
    "        \n",
    "        Your task is to determine whether the retrieved document is relevant to the user's question. Follow these steps:\n",
    "\n",
    "        1. Analyze the document and question for keyword matches and semantic relevance.\n",
    "        2. Consider any information in the document that could be helpful in answering the user's question, even if it's not a direct match.\n",
    "        3. Make a decision on relevance, erring on the side of relevance if there's any doubt.\n",
    "        4. Provide your assessment in the specified JSON format.\n",
    "        \n",
    "        Before making your final decision, wrap your thought process in <relevance_assessment> tags:\n",
    "        \n",
    "        <relevance_assessment>\n",
    "        1. Quote any relevant parts of the document and the question, highlighting keyword matches.\n",
    "        2. Describe any semantic connections or relevant information found in the document.\n",
    "        3. List arguments for considering the document relevant.\n",
    "        4. List arguments for considering the document not relevant.\n",
    "        5. Explain your reasoning for your final decision on relevance.\n",
    "        </relevance_assessment>\n",
    "        \n",
    "        After your assessment, provide your final decision in JSON format. The JSON must contain a single key \"relevant\" with a value of either \"yes\" or \"no\". For example:\n",
    "\n",
    "        {{\n",
    "          \"relevant\": \"yes\"\n",
    "        }}\n",
    "        \n",
    "        or\n",
    "        \n",
    "        {{\n",
    "          \"relevant\": \"no\"\n",
    "        }}\n",
    "        \n",
    "        Remember, it's important to consider both direct keyword matches and broader semantic relevance. If the document contains any information that could be helpful in addressing the user's question, even indirectly, it should be considered relevant.\n",
    "        \n",
    "        Wrap your answer in <output> tag.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    question = state[\"messages\"][0]\n",
    "    documents = state[\"messages\"][-1].content\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"documents\": documents,\n",
    "            \"query\": question,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    filtered_response = extract_json_output(response)\n",
    "    if filtered_response[\"relevant\"] == \"yes\":\n",
    "        return \"generate\"\n",
    "    elif filtered_response[\"relevant\"] == \"no\":\n",
    "        del state[\"messages\"][-1]\n",
    "        return \"rewrite_documents\"\n",
    "    \n",
    "\n",
    "def rewrite_documents(state: MessagesState):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an advanced language model tasked with improving user queries to enhance document retrieval and overall conversation quality. Your goal is to analyze the initial query and conversation history, understand the underlying semantic intent, and formulate an improved question.\n",
    "\n",
    "        Here is the history of the conversation:\n",
    "        \n",
    "        <conversation_history>\n",
    "        {messages}\n",
    "        </conversation_history>\n",
    "        \n",
    "        And here is the initial query from the user:\n",
    "        \n",
    "        <initial_query>\n",
    "        {query}\n",
    "        </initial_query>\n",
    "        \n",
    "        Please follow these steps to formulate an improved question:\n",
    "        \n",
    "        1. Analyze the conversation history and initial query.\n",
    "        2. Identify the underlying semantic intent or meaning behind the user's question.\n",
    "        3. Consider any additional context or information provided in the conversation history that could help clarify or refine the query.\n",
    "        4. Formulate an improved question that:\n",
    "           - Captures the core intent of the original query\n",
    "           - Incorporates relevant context from the conversation history\n",
    "           - Is more precise, clear, and likely to yield better document retrieval results\n",
    "           - Maintains the original topic and purpose of the query\n",
    "        \n",
    "        Before providing the final improved question, please wrap your thought process inside <query_improvement_process> tags. This will help ensure a thorough interpretation of the query and conversation context. In this process:\n",
    "        \n",
    "        1. Identify and list key topics/themes from the conversation history.\n",
    "        2. Quote relevant parts of the conversation history that provide context for the query.\n",
    "        3. Break down the initial query into its core components.\n",
    "        4. Consider how the conversation history might influence or refine each component of the query.\n",
    "        5. Explain how you arrived at the improved question based on this analysis.\n",
    "        \n",
    "        Output Format:\n",
    "        After your analysis, provide only the improved question wraped in <output> without any additional explanation or text.\n",
    "        \n",
    "        Example output structure:\n",
    "        \n",
    "        <query_improvement_process>\n",
    "        [Your detailed analysis of the conversation history and initial query, following the steps outlined above]\n",
    "        </query_improvement_process>\n",
    "        \n",
    "        <output>\n",
    "        [Improved question goes here, without any tags or additional text]\n",
    "        </output>\n",
    "        \n",
    "        Please proceed with your query improvement process and improved question formulation.      \n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser = StrOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    question = state[\"messages\"][0]\n",
    "    messages = state[\"messages\"][-3]\n",
    "    \n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"query\": question,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [extract_str_output(response)]}\n",
    "    \n",
    "\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(rewrite_documents)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    evaluate_documents,\n",
    "    {\"generate\": \"generate\", \"rewrite_documents\": \"rewrite_documents\"},\n",
    ")\n",
    "graph_builder.add_edge(\"rewrite_documents\", \"query_or_respond\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n"
   ],
   "id": "153ee4ccda0e2978",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:02:38.190127Z",
     "start_time": "2024-12-05T16:02:37.442697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_message = \"Hello\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ],
   "id": "9732a86e3114b0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Hello\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T17:22:26.982683Z",
     "start_time": "2024-12-05T17:22:20.260477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_message = (\"About what paper?\")\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ],
   "id": "8ded9dd24fa86f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "About what paper?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_U8KbOM4sOQUvAH0EVSK2TXjQ)\n",
      " Call ID: call_U8KbOM4sOQUvAH0EVSK2TXjQ\n",
      "  Args:\n",
      "    query: About what paper?\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 0}\n",
      "Content: 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\n",
      "For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 12, 2024\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 0}\n",
      "Content: I. INTRODUCTION\n",
      "Money laundering is a globally challenging economic\n",
      "concern. TheUN Vienna 1988 Convention describes it as\n",
      "‘‘the conversion or transfer of property, knowing that such\n",
      "property is derived from any offence (s), to conceal or\n",
      "The associate editor coordinating the review of this manuscript and\n",
      "approving it for publication was Ines Domingues\n",
      ".\n",
      "disguise the illicit origin of the property or of assisting any\n",
      "person who is involved in such offence (s) to evade the legal\n",
      "consequences of his actions. ’’. Being a global issue, money\n",
      "laundering results in approximately 0.8 to 2.0$ trillion being\n",
      "laundered every year, which equates to 2 to 5% of the world’s\n",
      "GDP [1], [2]. Being a global issue, money laundering results\n",
      "in approximately 0.8 to 2.0$ trillion being laundered every\n",
      "year, which equates to 2 to 5% of the world’s GDP [2].\n",
      "50012\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 0}\n",
      "Content: Received 20 February 2024, accepted 19 March 2024, date of publication 1 April 2024, date of current version 12 April 2024.\n",
      "Digital Object Identifier 10.1 109/ACCESS.2024.3383784\n",
      "Scalable Semi-Supervised Graph Learning\n",
      "Techniques for Anti Money Laundering\n",
      "MD. REZAUL KARIM\n",
      " 1,2, FELIX HERMSEN1,2, SISAY ADUGNA CHALA1,\n",
      "PAOLA DE PERTHUIS3,4, AND AVIKARSHA MANDAL\n",
      "2\n",
      "1Information Systems and Databases, RWTH Aachen University, 52074 Aachen, Germany\n",
      "2Department of Data Science and Artificial Intelligence, Fraunhofer FIT, 53757 Sankt Augustin, Germany\n",
      "3École Normale Supérieure (ENS), 75005 Paris, France\n",
      "4Cosmian, 75008 Paris, France\n",
      "Corresponding author: Md. Rezaul Karim (rezaul.karim@rwth-aachen.de)\n",
      "This work was supported by the Agence Nationale de la Recherche (ANR) and the Bundesministerium für Bildung und Forschung\n",
      "(BMBF) under the Franco-German AI Call for the project ‘‘CRYPTO4GRAPH-AI’’, grant number 01IS21100A.\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 16}\n",
      "Content: inference attacks on machine learning: A survey,’’ ACM Comput. Surv.,\n",
      "vol. 54, no. 11s, pp. 1–37, Jan. 2022.\n",
      "[44] P. de Perthuis and D. Pointcheval, ‘‘Two-client inner-product func-\n",
      "tional encryption with an application to money-laundering detection,’’\n",
      "inProc. ACM SIGSAC Conf. Comput. Commun. Secur. , Nov. 2022,\n",
      "pp. 725–737.\n",
      "50028 VOLUME 12, 2024\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "The paper discusses scalable semi-supervised graph learning techniques for anti-money laundering. It highlights the global challenge of money laundering, its economic impact, and the project's support from the Agence Nationale de la Recherche and the Bundesministerium für Bildung und Forschung. The authors include researchers from RWTH Aachen University and other institutions in France.\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9d0a02f7d266509f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
