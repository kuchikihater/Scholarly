{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T17:22:13.936129Z",
     "start_time": "2024-12-05T17:22:13.923469Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing_extensions import List, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T17:29:21.915174Z",
     "start_time": "2024-12-05T17:29:17.271703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_json_output(response: str):\n",
    "    json_match = re.search(r\"<output>(.*?)</output>\", response, re.DOTALL)\n",
    "    json_string = json_match.group(1).strip()\n",
    "    parsed_json = json.loads(json_string)\n",
    "    return parsed_json\n",
    "    \n",
    "def extract_str_output(response: str):\n",
    "    str_match = re.search(r\"<output>(.*?)</output>\", response, re.DOTALL)\n",
    "    str_string = str_match.group(1).strip()\n",
    "    return str_string\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "loader = PyPDFLoader(file_path=\"data/AML_IEEE_ACCESS_2024.pdf\", extract_images=True)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "docs = loader.load()\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "db = DocArrayInMemorySearch.from_documents(all_splits, embeddings)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    system_instruction = (\n",
    "        \"You are an assistant specialized in analyzing research papers. You can answer questions about the paper's content, structure, and specific details. Additionally, you have access to a retrieval tool connected to a vector database, which allows you to search for and retrieve relevant context from the paper when necessary. If a user's query includes the word 'this,' prioritize using the retrieval tool to find the most accurate information.\"\n",
    "    )\n",
    "\n",
    "    system_message = {\"role\": \"system\", \"content\": system_instruction}\n",
    "    \n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke([system_message, user_message])\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "def evaluate_documents(state: MessagesState):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an expert document assessor tasked with determining the relevance of a retrieved document to a user's question. Your goal is to provide an accurate relevance assessment based on both keyword matches and semantic understanding.\n",
    "\n",
    "        First, carefully review the following information:\n",
    "        \n",
    "        1. Retrieved Documents:\n",
    "        <retrieved documents>\n",
    "        {documents}\n",
    "        </retrieved documents>\n",
    "        \n",
    "        2. Here is the user question:\n",
    "        <question>\n",
    "        {query}\n",
    "        </question>\n",
    "        \n",
    "        Your task is to determine whether the retrieved document is relevant to the user's question. Follow these steps:\n",
    "\n",
    "        1. Analyze the document and question for keyword matches and semantic relevance.\n",
    "        2. Consider any information in the document that could be helpful in answering the user's question, even if it's not a direct match.\n",
    "        3. Make a decision on relevance, erring on the side of relevance if there's any doubt.\n",
    "        4. Provide your assessment in the specified JSON format.\n",
    "        \n",
    "        Before making your final decision, wrap your thought process in <relevance_assessment> tags:\n",
    "        \n",
    "        <relevance_assessment>\n",
    "        1. Quote any relevant parts of the document and the question, highlighting keyword matches.\n",
    "        2. Describe any semantic connections or relevant information found in the document.\n",
    "        3. List arguments for considering the document relevant.\n",
    "        4. List arguments for considering the document not relevant.\n",
    "        5. Explain your reasoning for your final decision on relevance.\n",
    "        </relevance_assessment>\n",
    "        \n",
    "        After your assessment, provide your final decision in JSON format. The JSON must contain a single key \"relevant\" with a value of either \"yes\" or \"no\". For example:\n",
    "\n",
    "        {{\n",
    "          \"relevant\": \"yes\"\n",
    "        }}\n",
    "        \n",
    "        or\n",
    "        \n",
    "        {{\n",
    "          \"relevant\": \"no\"\n",
    "        }}\n",
    "        \n",
    "        Remember, it's important to consider both direct keyword matches and broader semantic relevance. If the document contains any information that could be helpful in addressing the user's question, even indirectly, it should be considered relevant.\n",
    "        \n",
    "        Wrap your answer in <output> tag.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    question = state[\"messages\"][0]\n",
    "    documents = state[\"messages\"][-1].content\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"documents\": documents,\n",
    "            \"query\": question,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    filtered_response = extract_json_output(response)\n",
    "    if filtered_response[\"relevant\"] == \"yes\":\n",
    "        return \"generate\"\n",
    "    elif filtered_response[\"relevant\"] == \"no\":\n",
    "        del state[\"messages\"][-1]\n",
    "        return \"rewrite_documents\"\n",
    "    \n",
    "\n",
    "def rewrite_documents(state: MessagesState):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an advanced language model tasked with improving user queries to enhance document retrieval and overall conversation quality. Your goal is to analyze the initial query and conversation history, understand the underlying semantic intent, and formulate an improved question.\n",
    "\n",
    "        Here is the history of the conversation:\n",
    "        \n",
    "        <conversation_history>\n",
    "        {messages}\n",
    "        </conversation_history>\n",
    "        \n",
    "        And here is the initial query from the user:\n",
    "        \n",
    "        <initial_query>\n",
    "        {query}\n",
    "        </initial_query>\n",
    "        \n",
    "        Please follow these steps to formulate an improved question:\n",
    "        \n",
    "        1. Analyze the conversation history and initial query.\n",
    "        2. Identify the underlying semantic intent or meaning behind the user's question.\n",
    "        3. Consider any additional context or information provided in the conversation history that could help clarify or refine the query.\n",
    "        4. Formulate an improved question that:\n",
    "           - Captures the core intent of the original query\n",
    "           - Incorporates relevant context from the conversation history\n",
    "           - Is more precise, clear, and likely to yield better document retrieval results\n",
    "           - Maintains the original topic and purpose of the query\n",
    "        \n",
    "        Before providing the final improved question, please wrap your thought process inside <query_improvement_process> tags. This will help ensure a thorough interpretation of the query and conversation context. In this process:\n",
    "        \n",
    "        1. Identify and list key topics/themes from the conversation history.\n",
    "        2. Quote relevant parts of the conversation history that provide context for the query.\n",
    "        3. Break down the initial query into its core components.\n",
    "        4. Consider how the conversation history might influence or refine each component of the query.\n",
    "        5. Explain how you arrived at the improved question based on this analysis.\n",
    "        \n",
    "        Output Format:\n",
    "        After your analysis, provide only the improved question wraped in <output> without any additional explanation or text.\n",
    "        \n",
    "        Example output structure:\n",
    "        \n",
    "        <query_improvement_process>\n",
    "        [Your detailed analysis of the conversation history and initial query, following the steps outlined above]\n",
    "        </query_improvement_process>\n",
    "        \n",
    "        <output>\n",
    "        [Improved question goes here, without any tags or additional text]\n",
    "        </output>\n",
    "        \n",
    "        Please proceed with your query improvement process and improved question formulation.      \n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser = StrOutputParser()\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    question = state[\"messages\"][0]\n",
    "    messages = state[\"messages\"][-3]\n",
    "    \n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"query\": question,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [extract_str_output(response)]}\n",
    "    \n",
    "\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(rewrite_documents)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    evaluate_documents,\n",
    "    {\"generate\": \"generate\", \"rewrite_documents\": \"rewrite_documents\"},\n",
    ")\n",
    "graph_builder.add_edge(\"rewrite_documents\", \"query_or_respond\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n"
   ],
   "id": "153ee4ccda0e2978",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:02:38.190127Z",
     "start_time": "2024-12-05T16:02:37.442697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_message = \"Hello\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ],
   "id": "9732a86e3114b0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Hello\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T17:29:30.804389Z",
     "start_time": "2024-12-05T17:29:21.918691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_message = (\"About what this paper?\")\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ],
   "id": "8ded9dd24fa86f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "About what this paper?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_kdAy1dnNmqYi2fPzcduxEdtJ)\n",
      " Call ID: call_kdAy1dnNmqYi2fPzcduxEdtJ\n",
      "  Args:\n",
      "    query: about this paper\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 15}\n",
      "Content: of an AML model. Nevertheless, we hope the approach\n",
      "presented in this paper will make non-trivial contributions\n",
      "and give network security and financial crime analysis some\n",
      "insights into how to employ semi-supervised graph learning\n",
      "VOLUME 12, 2024 50027\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 0}\n",
      "Content: 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\n",
      "For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 12, 2024\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 0}\n",
      "Content: Received 20 February 2024, accepted 19 March 2024, date of publication 1 April 2024, date of current version 12 April 2024.\n",
      "Digital Object Identifier 10.1 109/ACCESS.2024.3383784\n",
      "Scalable Semi-Supervised Graph Learning\n",
      "Techniques for Anti Money Laundering\n",
      "MD. REZAUL KARIM\n",
      " 1,2, FELIX HERMSEN1,2, SISAY ADUGNA CHALA1,\n",
      "PAOLA DE PERTHUIS3,4, AND AVIKARSHA MANDAL\n",
      "2\n",
      "1Information Systems and Databases, RWTH Aachen University, 52074 Aachen, Germany\n",
      "2Department of Data Science and Artificial Intelligence, Fraunhofer FIT, 53757 Sankt Augustin, Germany\n",
      "3École Normale Supérieure (ENS), 75005 Paris, France\n",
      "4Cosmian, 75008 Paris, France\n",
      "Corresponding author: Md. Rezaul Karim (rezaul.karim@rwth-aachen.de)\n",
      "This work was supported by the Agence Nationale de la Recherche (ANR) and the Bundesministerium für Bildung und Forschung\n",
      "(BMBF) under the Franco-German AI Call for the project ‘‘CRYPTO4GRAPH-AI’’, grant number 01IS21100A.\n",
      "\n",
      "Source: {'source': 'data/AML_IEEE_ACCESS_2024.pdf', 'page': 2}\n",
      "Content: to train tree-based ensemble classifiers such as random\n",
      "forest (RF), extreme gradient boosted trees (XGBoost), and\n",
      "light gradient boosted machine (LightGBM) that predict the\n",
      "suspiciousness of a target node in potential money laundering\n",
      "activities based on its direct or indirect connections to nodes\n",
      "that are known to be suspicious. The overall contributions of\n",
      "this paper can be summarized as follows:\n",
      "• We address a globally challenging economic concern\n",
      "-money laundering with a view to its criticality and\n",
      "importance towards deploying scalable and robust AML\n",
      "models into real financial systems.\n",
      "• We employ state-of-the-art (SOTA) semi-supervised\n",
      "learning techniques on transaction graphs, where\n",
      "both spatial and temporal information, together with\n",
      "50014 VOLUME 12, 2024\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "The paper discusses scalable semi-supervised graph learning techniques for anti-money laundering (AML) efforts. It focuses on employing state-of-the-art learning methods to enhance the detection of suspicious activities in financial systems by analyzing transaction graphs. The authors aim to contribute insights into network security and financial crime analysis.\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9d0a02f7d266509f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
